{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 0.20.2\n",
      "keras version: 2.2.4\n",
      "tensorflow version: 1.12.0\n",
      "pandas version: 0.23.4\n",
      "numpy version: 1.15.4\n",
      "python version: 3.6.7 |Anaconda custom (64-bit)| (default, Oct 23 2018, 14:01:38) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hiauantie',\n",
       " '.',\n",
       " 'im',\n",
       " 'mother',\n",
       " 'citing',\n",
       " ',',\n",
       " 'okay',\n",
       " 'see',\n",
       " '.',\n",
       " 'kanea',\n",
       " '?',\n",
       " '?',\n",
       " '?',\n",
       " 'hi']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###now try to use keras to train the model\n",
    "###try deep learning \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pickle\n",
    "import sklearn\n",
    "import sys\n",
    "import cleanup\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "####use a pretrained wordembedding\n",
    "print(\"sklearn version:\",sklearn.__version__)\n",
    "print(\"keras version:\",keras.__version__)\n",
    "print(\"tensorflow version:\",tf.__version__)\n",
    "print(\"pandas version:\",pd.__version__)\n",
    "print(\"numpy version:\",np.__version__)\n",
    "print(\"python version:\",sys.version)\n",
    "clean=cleanup.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"complete_math_arxiv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to preprocess the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cat={}\n",
    "for i in df.index:\n",
    "    paper_cats=df.loc[i,\"categories\"]\n",
    "    for cat in paper_cats:\n",
    "        if cat in all_cat:\n",
    "            all_cat[cat]+=1\n",
    "        else:\n",
    "            all_cat[cat]=1\n",
    "\n",
    "list_of_all_cat=sorted(list(all_cat.items()),key=lambda x: x[1], reverse=True)            \n",
    "all_cats=list(zip(*list_of_all_cat))[0]\n",
    "index_to_cat={}\n",
    "for i in range(len(all_cats)):\n",
    "    index_to_cat[i]=all_cats[i]    \n",
    "def preprocess():\n",
    "    def list_of_authors(text):\n",
    "        names=[]\n",
    "        name_list=text.split(',')\n",
    "        for name in name_list:\n",
    "            name=name.strip()\n",
    "            if name[0]=='[':\n",
    "                name=name[1:]\n",
    "            if name[-1]==']':\n",
    "                name=name[:-1]\n",
    "            name=name[1:-1]\n",
    "            names+=[name]\n",
    "        return names    \n",
    "\n",
    "\n",
    "    def list_of_categories(text):\n",
    "        pat=re.compile(r'math.[A-Z][A-Z]')\n",
    "        return pat.findall(text)\n",
    "\n",
    "    ##now we can make them into a list\n",
    "    df['categories']=df['categories'].apply(lambda x: list_of_categories(x))\n",
    "    df['authors']=df['authors'].apply(lambda x: list_of_authors(x))\n",
    "    df['created']=df['created'].apply(lambda x:datetime.datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "    if \"clean_abstract\" not in df.columns:\n",
    "        df['clean_abstract']=df['abstract'].apply(lambda x: clean.transform(x))\n",
    "    for i in range(len(index_to_cat)):\n",
    "        df[index_to_cat[i]]=df[\"categories\"].apply(lambda x: 1 if index_to_cat[i] in x else 0)\n",
    "\n",
    "preprocess()        \n",
    "###make \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
