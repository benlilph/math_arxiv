{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 0.20.2\n",
      "keras version: 2.2.4\n",
      "tensorflow version: 1.12.0\n",
      "pandas version: 0.23.4\n",
      "numpy version: 1.15.4\n",
      "python version: 3.6.7 |Anaconda custom (64-bit)| (default, Oct 23 2018, 14:01:38) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "###now try to use keras to train the model\n",
    "###try deep learning \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pickle\n",
    "import sklearn\n",
    "import sys\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import gensim \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import statistics\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "print(\"sklearn version:\",sklearn.__version__)\n",
    "print(\"keras version:\",keras.__version__)\n",
    "print(\"tensorflow version:\",tf.__version__)\n",
    "print(\"pandas version:\",pd.__version__)\n",
    "print(\"numpy version:\",np.__version__)\n",
    "print(\"python version:\",sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w{3,}')\n",
    "def simple_cleaning(test_str):\n",
    "    test_str = re.sub(r\"\\n\", \" \", test_str, 0, re.MULTILINE)\n",
    "    test_str = re.sub(r\"(\\$+)(?:(?!\\1)[\\s\\S])*\\1\", \"\", test_str, 0, re.MULTILINE)\n",
    "    test_str = re.sub(r\"-\", \" \", test_str, 0, re.MULTILINE)\n",
    "    test_str = re.sub(r\"[\\\\'/{}\\\":\\(\\).,]\", \"\", test_str, 0, re.MULTILINE)\n",
    "    test_str = test_str.lower()\n",
    "    return test_str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"complete_math_arxiv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['simple_abstract']=df['abstract'].apply(lambda x:simple_cleaning(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let we prepare the list of sent, each sent is a list of words\n",
    "## we will just skip stopwords and any words less than length 3\n",
    "###  use snowballstemmer to further clean the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "tokenizer_regex = RegexpTokenizer(r'\\w{3,}')\n",
    "df['list_simple_abstract']=df['simple_abstract'].apply(lambda x: tokenizer_regex.tokenize(x))\n",
    "from nltk.corpus import stopwords\n",
    "STOPS=set(stopwords.words('english'))\n",
    "df['list_simple_abstract'] = df['list_simple_abstract'].apply(lambda x : [stemmer.stem(y.strip()) for y in x if y not in STOPS])\n",
    "df['list_simple_abstract'] = df['list_simple_abstract'].apply(lambda x : [stemmer.stem(y.strip()) for y in x if (y not in STOPS) and (len(re.compile(r'\\d').findall(y))==0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['simple_abstract']=df['list_simple_abstract'].apply(lambda x :\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Preprocess\n",
    "def list_of_authors(text):\n",
    "    names=[]\n",
    "    name_list=text.split(',')\n",
    "    for name in name_list:\n",
    "        name=name.strip()\n",
    "        if name[0]=='[':\n",
    "            name=name[1:]\n",
    "        if name[-1]==']':\n",
    "            name=name[:-1]\n",
    "        name=name[1:-1]\n",
    "        names+=[name]\n",
    "    return names    \n",
    "\n",
    "\n",
    "def list_of_categories(text):\n",
    "    pat=re.compile(r'math.[A-Z][A-Z]')\n",
    "    return pat.findall(text)\n",
    "\n",
    "df['categories']=df['categories'].apply(lambda x: list_of_categories(x))\n",
    "df['authors']=df['authors'].apply(lambda x: list_of_authors(x))\n",
    "df['created']=df['created'].apply(lambda x:datetime.datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "if \"clean_abstract\" not in df.columns:\n",
    "    df['clean_abstract']=df['abstract'].apply(lambda x: clean.transform(x))\n",
    "      \n",
    "all_cat={}\n",
    "for i in df.index:\n",
    "    paper_cats=df.loc[i,\"categories\"]\n",
    "    for cat in paper_cats:\n",
    "        if cat in all_cat:\n",
    "            all_cat[cat]+=1\n",
    "        else:\n",
    "            all_cat[cat]=1\n",
    "\n",
    "list_of_all_cat=sorted(list(all_cat.items()),key=lambda x: x[1], reverse=True)            \n",
    "all_cats=list(zip(*list_of_all_cat))[0]\n",
    "index_to_cat={}\n",
    "for i in range(len(all_cats)):\n",
    "    index_to_cat[i]=all_cats[i]    \n",
    "\n",
    "for i in range(len(index_to_cat)):\n",
    "    df[index_to_cat[i]]=df[\"categories\"].apply(lambda x: 1 if index_to_cat[i] in x else 0)\n",
    "\n",
    "      \n",
    "###make \n",
    "\n",
    "df=df[df['clean_abstract'].apply(lambda x : type(x)!=type(3.0))]\n",
    "df=df[df['categories'].apply(lambda x : len(x)>0)]    \n",
    "df=df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df[df['created']<=datetime.datetime(year=2017,month=1,day=1)]\n",
    "valid=df[(df['created']<datetime.datetime(year=2018,month=1,day=1))&(df['created']>datetime.datetime(year=2017,month=1,day=1))]\n",
    "test=df[df['created']>=datetime.datetime(year=2018,month=1,day=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "### Now we can try FastText\n",
    "### One cool thing about FastText is that it doesn't require us to see the wrod before.\n",
    "### Now we can directly process each input text as a matrix of the form (max_len,max_features), where max_len is the padded sequence length and max_features is the dim of embedding matrix of fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "max_feature=100\n",
    "model_ted = FastText(list(df['list_simple_abstract']), size=100, window=3, min_count=2, workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ted.save(\"FastText.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ted = FastText.load(\"FastText.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = list(model_ted.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=[]\n",
    "for i in df.index:\n",
    "    all_words+=df.loc[i,'list_simple_abstract']\n",
    "all_words=set(all_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_embedding=np.zeros(shape=(len(all_words),max_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "def embedding_fasttext(list_words,max_len,max_features):\n",
    "    mat=np.zeros(shape=(max_len,max_features))\n",
    "    temp_list_words=[]\n",
    "    for x in list_words:\n",
    "        if x.strip() in model_ted.wv:\n",
    "            temp_list_words+=[x.strip()]\n",
    "    list_words=temp_list_words\n",
    "    \n",
    "    if len(list_words)>=200:\n",
    "        for i in range(200):\n",
    "            mat[i,:]=model_ted.wv[list_words[i]]\n",
    "    else:\n",
    "        length=len(list_words)\n",
    "        for i in range(200-length,200):\n",
    "            mat[i,:]=model_ted.wv[list_words[i-(200-length)]]\n",
    "    return mat        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(list(df['simple_abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding():\n",
    "    embedding=np.zeros(shape=(len(tokenizer.word_index)+1,100))\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if word in model_ted.wv:\n",
    "            embedding[index,]=model_ted.wv[word]\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = build_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now we can train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314424\n",
      "37049\n",
      "36338\n"
     ]
    }
   ],
   "source": [
    "train=df[df['created']<=datetime.datetime(year=2017,month=1,day=1)]\n",
    "valid=df[(df['created']<datetime.datetime(year=2018,month=1,day=1))&(df['created']>datetime.datetime(year=2017,month=1,day=1))]\n",
    "test=df[df['created']>=datetime.datetime(year=2018,month=1,day=1)]\n",
    "print(len(train))\n",
    "print(len(valid))\n",
    "print(len(test))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train['simple_abstract'])\n",
    "list_tokenized_valid = tokenizer.texts_to_sequences(valid['simple_abstract'])\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(test['simple_abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134544, 100)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.56620e+04, 1.58834e+05, 8.64950e+04, 3.77250e+04, 1.49220e+04,\n",
       "        4.05600e+03, 9.50000e+01, 1.30000e+01, 5.00000e+00, 4.00000e+00]),\n",
       " array([  0. ,  30.9,  61.8,  92.7, 123.6, 154.5, 185.4, 216.3, 247.2,\n",
       "        278.1, 309. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAGCFJREFUeJzt3X+s3fV93/Hna3Ygv5oYwiVjtjU7\njdvVQVlDPOItW5ThFgypYiaBZLQNK7NkjUKXTusas0ilS4IEXVc2JEJFg4eJIgyj6bAWM9cCsmhS\n+HEJBHAo8S0wuIHimxooXRSok/f+OJ+bnFyf6/vlHodzr/N8SEfn+31/P9/v9/Pha/vF98c5J1WF\nJEld/K1Rd0CStHgYGpKkzgwNSVJnhoYkqTNDQ5LUmaEhSerM0JAkdWZoSJI6MzQkSZ0tHXUHjrVT\nTjmlVq1aNepuSNKi8uCDD363qsbmanfchcaqVasYHx8fdTckaVFJ8n+7tPPylCSpM0NDktSZoSFJ\n6szQkCR1NmdoJNmR5GCSx2bUfyPJE0n2J/m9vvrlSSbasnP66htbbSLJ9r766iT3JTmQ5NYkJ7T6\niW1+oi1fdSwGLEmavy5nGjcBG/sLSf4psAl4f1W9D/j9Vl8LbAbe19b5fJIlSZYA1wHnAmuBi1pb\ngKuBa6pqDfAisLXVtwIvVtV7gWtaO0nSCM0ZGlX1NeDQjPIlwFVV9Wprc7DVNwG7qurVqnoKmADO\nbK+Jqnqyql4DdgGbkgQ4C7i9rb8TOL9vWzvb9O3AhtZekjQi872n8QvAP2mXjf53kn/Q6suBZ/va\nTbbabPV3AS9V1eEZ9Z/YVlv+cmt/hCTbkownGZ+amprnkCRJc5lvaCwFTgLWA/8euK2dBQw6E6h5\n1Jlj2U8Wq26oqnVVtW5sbM4PNEqS5mm+nwifBL5cVQXcn+SHwCmtvrKv3QrguTY9qP5dYFmSpe1s\nor/99LYmkywF3smRl8mOG6u2f2Uk+336qo+NZL+SFqf5nmn8D3r3IkjyC8AJ9AJgN7C5Pfm0GlgD\n3A88AKxpT0qdQO9m+e4WOvcAF7TtbgHuaNO72zxt+d2tvSRpROY800hyC/BR4JQkk8AVwA5gR3sM\n9zVgS/sHfX+S24BvAYeBS6vqB207lwF7gSXAjqra33bxKWBXks8BDwE3tvqNwBeTTNA7w9h8DMYr\nSRrCnKFRVRfNsuhfzNL+SuDKAfU9wJ4B9SfpPV01s/594MK5+idJeuP4iXBJUmeGhiSpM0NDktSZ\noSFJ6szQkCR1ZmhIkjozNCRJnRkakqTODA1JUmeGhiSpM0NDktSZoSFJ6szQkCR1ZmhIkjozNCRJ\nnRkakqTODA1JUmdzhkaSHUkOtp92nbnst5JUklPafJJcm2QiySNJzuhruyXJgfba0lf/YJJH2zrX\nJkmrn5xkX2u/L8lJx2bIkqT56nKmcROwcWYxyUrgV4Fn+srnAmvaaxtwfWt7Mr3fFv8QvZ92vaIv\nBK5vbafXm97XduCuqloD3NXmJUkjNGdoVNXXgEMDFl0D/DZQfbVNwM3Vcy+wLMlpwDnAvqo6VFUv\nAvuAjW3ZO6rq61VVwM3A+X3b2tmmd/bVJUkjMq97Gkk+Dnynqr45Y9Fy4Nm++clWO1p9ckAd4N1V\n9TxAez/1KP3ZlmQ8yfjU1NQ8RiRJ6uJ1h0aStwKfBn5n0OIBtZpH/XWpqhuqal1VrRsbG3u9q0uS\nOprPmcbPA6uBbyZ5GlgBfCPJ36Z3prCyr+0K4Lk56isG1AFeaJevaO8H59FXSdIx9LpDo6oerapT\nq2pVVa2i9w//GVX1F8Bu4OL2FNV64OV2aWkvcHaSk9oN8LOBvW3ZK0nWt6emLgbuaLvaDUw/ZbWl\nry5JGpEuj9zeAnwd+MUkk0m2HqX5HuBJYAL4I+DXAarqEPBZ4IH2+kyrAVwCfKGt8+fAna1+FfCr\nSQ7Qe0rrqtc3NEnSsbZ0rgZVddEcy1f1TRdw6SztdgA7BtTHgdMH1P8S2DBX/yRJbxw/ES5J6szQ\nkCR1ZmhIkjozNCRJnRkakqTODA1JUmeGhiSpM0NDktSZoSFJ6szQkCR1ZmhIkjozNCRJnRkakqTO\nDA1JUmeGhiSpM0NDktSZoSFJ6qzLz73uSHIwyWN9tf+U5M+SPJLkT5Is61t2eZKJJE8kOaevvrHV\nJpJs76uvTnJfkgNJbk1yQquf2OYn2vJVx2rQkqT56XKmcROwcUZtH3B6Vb0f+DZwOUCStcBm4H1t\nnc8nWZJkCXAdcC6wFriotQW4GrimqtYALwLTv0G+FXixqt4LXNPaSZJGaM7QqKqvAYdm1P60qg63\n2XuBFW16E7Crql6tqqeACeDM9pqoqier6jVgF7ApSYCzgNvb+juB8/u2tbNN3w5saO0lSSNyLO5p\n/Cvgzja9HHi2b9lkq81WfxfwUl8ATdd/Yltt+cut/RGSbEsynmR8ampq6AFJkgYbKjSSfBo4DHxp\nujSgWc2jfrRtHVmsuqGq1lXVurGxsaN3WpI0b0vnu2KSLcCvARuqavof80lgZV+zFcBzbXpQ/bvA\nsiRL29lEf/vpbU0mWQq8kxmXySRJb6x5nWkk2Qh8Cvh4VX2vb9FuYHN78mk1sAa4H3gAWNOelDqB\n3s3y3S1s7gEuaOtvAe7o29aWNn0BcHdfOEmSRmDOM40ktwAfBU5JMglcQe9pqROBfe3e9L1V9a+r\nan+S24Bv0btsdWlV/aBt5zJgL7AE2FFV+9suPgXsSvI54CHgxla/Efhikgl6Zxibj8F4JUlDmDM0\nquqiAeUbB9Sm218JXDmgvgfYM6D+JL2nq2bWvw9cOFf/JElvHD8RLknqzNCQJHVmaEiSOjM0JEmd\nGRqSpM4MDUlSZ4aGJKkzQ0OS1JmhIUnqzNCQJHVmaEiSOjM0JEmdGRqSpM4MDUlSZ4aGJKkzQ0OS\n1JmhIUnqbM7QSLIjycEkj/XVTk6yL8mB9n5SqyfJtUkmkjyS5Iy+dba09geSbOmrfzDJo22da9N+\nP3a2fUiSRqfLmcZNwMYZte3AXVW1BrirzQOcC6xpr23A9dALAHq/Lf4hej/tekVfCFzf2k6vt3GO\nfUiSRmTO0KiqrwGHZpQ3ATvb9E7g/L76zdVzL7AsyWnAOcC+qjpUVS8C+4CNbdk7qurrVVXAzTO2\nNWgfkqQRme89jXdX1fMA7f3UVl8OPNvXbrLVjlafHFA/2j6OkGRbkvEk41NTU/MckiRpLsf6RngG\n1Goe9delqm6oqnVVtW5sbOz1ri5J6mi+ofFCu7REez/Y6pPAyr52K4Dn5qivGFA/2j4kSSMy39DY\nDUw/AbUFuKOvfnF7imo98HK7tLQXODvJSe0G+NnA3rbslSTr21NTF8/Y1qB9SJJGZOlcDZLcAnwU\nOCXJJL2noK4CbkuyFXgGuLA13wOcB0wA3wM+AVBVh5J8FnigtftMVU3fXL+E3hNabwHubC+Osg9J\n0ojMGRpVddEsizYMaFvApbNsZwewY0B9HDh9QP0vB+1DkjQ6fiJcktSZoSFJ6szQkCR1ZmhIkjoz\nNCRJnRkakqTODA1JUmeGhiSpM0NDktSZoSFJ6szQkCR1ZmhIkjqb8wsLdXxbtf0rI9nv01d9bCT7\nlTQcQ6PPqP4BlaTFwstTkqTODA1JUmeGhiSps6FCI8m/TbI/yWNJbkny5iSrk9yX5ECSW5Oc0Nqe\n2OYn2vJVfdu5vNWfSHJOX31jq00k2T5MXyVJw5t3aCRZDvwbYF1VnQ4sATYDVwPXVNUa4EVga1tl\nK/BiVb0XuKa1I8natt77gI3A55MsSbIEuA44F1gLXNTaSpJGZNjLU0uBtyRZCrwVeB44C7i9Ld8J\nnN+mN7V52vINSdLqu6rq1ap6CpgAzmyviap6sqpeA3a1tpKkEZl3aFTVd4DfB56hFxYvAw8CL1XV\n4dZsEljeppcDz7Z1D7f27+qvz1hntvoRkmxLMp5kfGpqar5DkiTNYZjLUyfR+z//1cDfAd5G71LS\nTDW9yizLXm/9yGLVDVW1rqrWjY2NzdV1SdI8DXN56leAp6pqqqr+Bvgy8I+AZe1yFcAK4Lk2PQms\nBGjL3wkc6q/PWGe2uiRpRIYJjWeA9Une2u5NbAC+BdwDXNDabAHuaNO72zxt+d1VVa2+uT1dtRpY\nA9wPPACsaU9jnUDvZvnuIforSRrSvL9GpKruS3I78A3gMPAQcAPwFWBXks+12o1tlRuBLyaZoHeG\nsbltZ3+S2+gFzmHg0qr6AUCSy4C99J7M2lFV++fbX0nS8Ib67qmqugK4Ykb5SXpPPs1s+33gwlm2\ncyVw5YD6HmDPMH2UJB07fiJcktSZoSFJ6szQkCR1ZmhIkjozNCRJnRkakqTODA1JUmeGhiSpM0ND\nktSZoSFJ6szQkCR1ZmhIkjozNCRJnRkakqTODA1JUmeGhiSpM0NDktTZUKGRZFmS25P8WZLHk/zD\nJCcn2ZfkQHs/qbVNkmuTTCR5JMkZfdvZ0tofSLKlr/7BJI+2da5tv0UuSRqRYc80/ivwv6rq7wF/\nH3gc2A7cVVVrgLvaPMC5wJr22gZcD5DkZHo/Gfshej8Te8V00LQ22/rW2zhkfyVJQ5h3aCR5B/AR\n4EaAqnqtql4CNgE7W7OdwPltehNwc/XcCyxLchpwDrCvqg5V1YvAPmBjW/aOqvp6VRVwc9+2JEkj\nMMyZxnuAKeC/JXkoyReSvA14d1U9D9DeT23tlwPP9q0/2WpHq08OqEuSRmSY0FgKnAFcX1UfAP4f\nP74UNcig+xE1j/qRG062JRlPMj41NXX0XkuS5m2Y0JgEJqvqvjZ/O70QeaFdWqK9H+xrv7Jv/RXA\nc3PUVwyoH6GqbqiqdVW1bmxsbIghSZKOZt6hUVV/ATyb5BdbaQPwLWA3MP0E1Bbgjja9G7i4PUW1\nHni5Xb7aC5yd5KR2A/xsYG9b9kqS9e2pqYv7tiVJGoGlQ67/G8CXkpwAPAl8gl4Q3ZZkK/AMcGFr\nuwc4D5gAvtfaUlWHknwWeKC1+0xVHWrTlwA3AW8B7mwvSdKIDBUaVfUwsG7Aog0D2hZw6Szb2QHs\nGFAfB04fpo+SpGPHT4RLkjozNCRJnRkakqTODA1JUmeGhiSpM0NDktSZoSFJ6szQkCR1ZmhIkjoz\nNCRJnRkakqTODA1JUmeGhiSpM0NDktSZoSFJ6szQkCR1ZmhIkjobOjSSLEnyUJL/2eZXJ7kvyYEk\nt7afgiXJiW1+oi1f1beNy1v9iSTn9NU3ttpEku3D9lWSNJxjcabxSeDxvvmrgWuqag3wIrC11bcC\nL1bVe4FrWjuSrAU2A+8DNgKfb0G0BLgOOBdYC1zU2kqSRmSo0EiyAvgY8IU2H+As4PbWZCdwfpve\n1OZpyze09puAXVX1alU9BUwAZ7bXRFU9WVWvAbtaW0nSiAx7pvFfgN8Gftjm3wW8VFWH2/wksLxN\nLweeBWjLX27tf1Sfsc5sdUnSiMw7NJL8GnCwqh7sLw9oWnMse731QX3ZlmQ8yfjU1NRRei1JGsYw\nZxofBj6e5Gl6l47OonfmsSzJ0tZmBfBcm54EVgK05e8EDvXXZ6wzW/0IVXVDVa2rqnVjY2NDDEmS\ndDTzDo2quryqVlTVKno3su+uqn8O3ANc0JptAe5o07vbPG353VVVrb65PV21GlgD3A88AKxpT2Od\n0Paxe779lSQNb+ncTV63TwG7knwOeAi4sdVvBL6YZILeGcZmgKran+Q24FvAYeDSqvoBQJLLgL3A\nEmBHVe3/KfRXI7Bq+1dGtu+nr/rYyPYtLXbHJDSq6qvAV9v0k/SefJrZ5vvAhbOsfyVw5YD6HmDP\nseijJGl4fiJcktSZoSFJ6szQkCR1ZmhIkjozNCRJnRkakqTODA1JUmeGhiSpM0NDktSZoSFJ6szQ\nkCR1ZmhIkjozNCRJnRkakqTODA1JUmeGhiSpM0NDktTZvEMjycok9yR5PMn+JJ9s9ZOT7EtyoL2f\n1OpJcm2SiSSPJDmjb1tbWvsDSbb01T+Y5NG2zrVJMsxgJUnDGeZM4zDw76rql4D1wKVJ1gLbgbuq\nag1wV5sHOBdY017bgOuhFzLAFcCH6P1M7BXTQdPabOtbb+MQ/ZUkDWneoVFVz1fVN9r0K8DjwHJg\nE7CzNdsJnN+mNwE3V8+9wLIkpwHnAPuq6lBVvQjsAza2Ze+oqq9XVQE3921LkjQCx+SeRpJVwAeA\n+4B3V9Xz0AsW4NTWbDnwbN9qk612tPrkgLokaUSGDo0kbwf+GPjNqvqrozUdUKt51Af1YVuS8STj\nU1NTc3VZkjRPQ4VGkjfRC4wvVdWXW/mFdmmJ9n6w1SeBlX2rrwCem6O+YkD9CFV1Q1Wtq6p1Y2Nj\nwwxJknQUwzw9FeBG4PGq+oO+RbuB6SegtgB39NUvbk9RrQdebpev9gJnJzmp3QA/G9jblr2SZH3b\n18V925IkjcDSIdb9MPAvgUeTPNxq/wG4CrgtyVbgGeDCtmwPcB4wAXwP+ARAVR1K8lnggdbuM1V1\nqE1fAtwEvAW4s70kSSMy79Coqv/D4PsOABsGtC/g0lm2tQPYMaA+Dpw+3z5Kko4tPxEuSepsmMtT\n0qK0avtXRrLfp6/62Ej2Kx1LnmlIkjozNCRJnRkakqTODA1JUmeGhiSpM0NDktSZoSFJ6szQkCR1\nZmhIkjozNCRJnRkakqTODA1JUmeGhiSpM0NDktSZX40uvUFG9ZXs4Ney69hZ8GcaSTYmeSLJRJLt\no+6PJP0sW9ChkWQJcB1wLrAWuCjJ2tH2SpJ+di3o0ADOBCaq6smqeg3YBWwacZ8k6WfWQg+N5cCz\nffOTrSZJGoGFfiM8A2p1RKNkG7Ctzf51kifmub9TgO/Oc92F4ngYAxwf41gwY8jV8151wYxhCMfD\nGOCnP46/26XRQg+NSWBl3/wK4LmZjarqBuCGYXeWZLyq1g27nVE6HsYAx8c4HMPCcDyMARbOOBb6\n5akHgDVJVic5AdgM7B5xnyTpZ9aCPtOoqsNJLgP2AkuAHVW1f8TdkqSfWQs6NACqag+w5w3a3dCX\nuBaA42EMcHyMwzEsDMfDGGCBjCNVR9xXliRpoIV+T0OStIAYGs1i/bqSJE8neTTJw0nGW+3kJPuS\nHGjvJ426n/2S7EhyMMljfbWBfU7Pte24PJLkjNH1/CfNMo7fTfKddjweTnJe37LL2zieSHLOaHr9\nY0lWJrknyeNJ9if5ZKsvqmNxlHEspmPx5iT3J/lmG8N/bPXVSe5rx+LW9kAQSU5s8xNt+ao3rLNV\n9TP/oneT/c+B9wAnAN8E1o66Xx37/jRwyoza7wHb2/R24OpR93NG/z4CnAE8NlefgfOAO+l9Zmc9\ncN+o+z/HOH4X+K0Bbde2P1cnAqvbn7clI+7/acAZbfrngG+3fi6qY3GUcSymYxHg7W36TcB97b/x\nbcDmVv9D4JI2/evAH7bpzcCtb1RfPdPoOd6+rmQTsLNN7wTOH2FfjlBVXwMOzSjP1udNwM3Vcy+w\nLMlpb0xPj26WccxmE7Crql6tqqeACXp/7kamqp6vqm+06VeAx+l948KiOhZHGcdsFuKxqKr66zb7\npvYq4Czg9lafeSymj9HtwIYkgz4MfcwZGj2L+etKCvjTJA+2T8YDvLuqnofeXyjg1JH1rrvZ+rwY\nj81l7fLNjr5Lgwt6HO3yxgfo/R/uoj0WM8YBi+hYJFmS5GHgILCP3hnQS1V1uDXp7+ePxtCWvwy8\n643op6HR0+nrShaoD1fVGfS+CfjSJB8ZdYeOscV2bK4Hfh74ZeB54D+3+oIdR5K3A38M/GZV/dXR\nmg6oLYgxwMBxLKpjUVU/qKpfpvfNF2cCvzSoWXsf2RgMjZ5OX1eyEFXVc+39IPAn9P6wvTB92aC9\nHxxdDzubrc+L6thU1QvtL/8PgT/ix5c9FuQ4kryJ3j+0X6qqL7fyojsWg8ax2I7FtKp6CfgqvXsa\ny5JMf56uv58/GkNb/k66XyodiqHRsyi/riTJ25L83PQ0cDbwGL2+b2nNtgB3jKaHr8tsfd4NXNye\n3FkPvDx96WQhmnGN/5/ROx7QG8fm9tTLamANcP8b3b9+7Rr4jcDjVfUHfYsW1bGYbRyL7FiMJVnW\npt8C/Aq9ezP3ABe0ZjOPxfQxugC4u9pd8Z+6UT4xsJBe9J4M+Ta964ifHnV/Ovb5PfSeAvkmsH+6\n3/Subd4FHGjvJ4+6rzP6fQu9ywV/Q+//mLbO1md6p+HXtePyKLBu1P2fYxxfbP18hN5f7NP62n+6\njeMJ4NwF0P9/TO+SxiPAw+113mI7FkcZx2I6Fu8HHmp9fQz4nVZ/D71AmwD+O3Biq7+5zU+05e95\no/rqJ8IlSZ15eUqS1JmhIUnqzNCQJHVmaEiSOjM0JEmdGRqSpM4MDUlSZ4aGJKmz/w9ji1xISGFG\nFwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_lengths=list(df['list_simple_abstract'].apply(lambda x:len(x)))\n",
    "plt.hist(all_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choose the padding length to be 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "X_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_valid = pad_sequences(list_tokenized_valid, maxlen=maxlen)\n",
    "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import GRU,GlobalAveragePooling1D,GlobalMaxPooling1D,Conv1D,concatenate\n",
    "num_class=len(index_to_cat)\n",
    "inp = Input(shape=(200, ))\n",
    "###then do embedding\n",
    "embed_size = 100\n",
    "max_features = 134544\n",
    "x = Embedding(max_features, embed_size,weights=[embedding_matrix],trainable=True)(inp)\n",
    "\n",
    "\n",
    "### then do LSTM\n",
    "x = Bidirectional(GRU(256,name='LSTM_layer',dropout=0.1, recurrent_dropout=0.1,return_sequences=True))(x)\n",
    "### 1D conv\n",
    "x = Conv1D(256, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "### maxpool\n",
    "#x = GlobalMaxPool1D()(x)\n",
    "### batchnor\n",
    "#x = BatchNormalization()(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "### dropout\n",
    "x = Dropout(0.1)(x)\n",
    "### relu\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "### dropout\n",
    "x = Dropout(0.1)(x)\n",
    "###\n",
    "#x = BatchNormalization()(x)\n",
    "x = Dense(num_class , activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, 200, 100)          13454400  \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 200, 512)          548352    \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 198, 256)          393472    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_11 (Glo (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 14,439,456\n",
      "Trainable params: 14,439,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.apply(lambda row:[row[index_to_cat[i]] for i in range(num_class)],axis=1)\n",
    "y_train = np.array(list(y_train))\n",
    "y_valid = valid.apply(lambda row:[row[index_to_cat[i]] for i in range(num_class)],axis=1)\n",
    "y_valid = np.array(list(y_valid))\n",
    "y_test = test.apply(lambda row:[row[index_to_cat[i]] for i in range(num_class)],axis=1)\n",
    "y_test = np.array(list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 314424 samples, validate on 37049 samples\n",
      "Epoch 1/10\n",
      "314424/314424 [==============================] - 11553s 37ms/step - loss: 0.0835 - categorical_accuracy: 0.6088 - val_loss: 0.0621 - val_categorical_accuracy: 0.6938\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06207, saving model to /Users/ben/Documents/NLP project/fasttext_embedding_max_pool_biGRU.h5\n",
      "Epoch 2/10\n",
      "314424/314424 [==============================] - 9607s 31ms/step - loss: 0.0677 - categorical_accuracy: 0.6709 - val_loss: 0.0593 - val_categorical_accuracy: 0.7012\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06207 to 0.05928, saving model to /Users/ben/Documents/NLP project/fasttext_embedding_max_pool_biGRU.h5\n",
      "Epoch 3/10\n",
      "314424/314424 [==============================] - 9554s 30ms/step - loss: 0.0627 - categorical_accuracy: 0.6888 - val_loss: 0.0599 - val_categorical_accuracy: 0.6989\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.05928\n",
      "Epoch 4/10\n",
      "314424/314424 [==============================] - 9713s 31ms/step - loss: 0.0581 - categorical_accuracy: 0.7068 - val_loss: 0.0600 - val_categorical_accuracy: 0.7055\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.05928\n",
      "Epoch 5/10\n",
      " 60160/314424 [====>.........................] - ETA: 2:06:08 - loss: 0.0544 - categorical_accuracy: 0.7192"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-307-24035d38fc6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcheckpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/fasttext_embedding_max_pool_biGRU.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2948\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2950\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2951\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2952\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2906\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2907\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2908\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2909\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath=cwd+\"/fasttext_embedding_max_pool_biGRU.h5\", verbose=1, save_best_only=True)\n",
    "history = model.fit(X_train,y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid),callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314424/314424 [==============================] - 3435s 11ms/step\n",
      "37049/37049 [==============================] - 452s 12ms/step\n",
      "Train: 0.740, Test: 0.705\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-308-847f44e1355e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train: %.3f, Test: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# plot training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "_, train_acc = model.evaluate(X_train, y_train, verbose=1)\n",
    "_, test_acc = model.evaluate(X_valid, y_valid, verbose=1)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model('fasttext_embedding_max_pool_biGRU.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36338/36338 [==============================] - 501s 14ms/step\n",
      "0.703258297092894\n"
     ]
    }
   ],
   "source": [
    "_, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AC for testing set: 0.5814849468875557\n"
     ]
    }
   ],
   "source": [
    "y_test_predict=model.predict(X_test)\n",
    "y_test_predict1=(y_test_predict>0.5).astype(int)\n",
    "for i in range(len(y_test_predict)):\n",
    "    best=y_test_predict[i].argmax()\n",
    "    y_test_predict1[i,best]=1\n",
    "\n",
    "\n",
    "acc=0\n",
    "for i in range(len(y_test_predict1)):\n",
    "    if (y_test_predict1[i]==y_test[i]).all():\n",
    "        acc+=1\n",
    "print(\"AC for testing set:\",acc/len(y_test))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
